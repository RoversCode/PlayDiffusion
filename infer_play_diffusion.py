#!/usr/bin/env python
# -*- encoding: utf-8 -*-
'''
@File    :   infer_play_diffusion.py
@Time    :   2025/06/21 17:24:05
@Author  :   ChengHee
@Version :   1.0
@Contact :   liujunjie199810@gmail.com
@Desc    :   PlayDiffusion inference script
'''

# here put the import lib
import os
import sys
import argparse
from typing import List, Dict, Any, Optional

sys.path.append("src")
import nemo.collections.asr as nemo_asr
from playdiffusion import PlayDiffusion, InpaintInput, TTSInput
import soundfile as sf


class PlayDiffusionInference:
    def __init__(self, asr_model_path: str = "modeldata/audio/asr/parakeet/parakeet-tdt-0.6b-v2.nemo"):
        """
        åˆå§‹åŒ–æ¨ç†å™¨
        
        Args:
            asr_model_path: ASRæ¨¡å‹è·¯å¾„
        """
        self.inpainter = PlayDiffusion()
        self.asr_model = nemo_asr.models.ASRModel.restore_from(asr_model_path)
        print(f"âœ… æ¨¡å‹åŠ è½½å®Œæˆ: {asr_model_path}")

    def run_asr(self, audio_path: str) -> tuple[str, str, List[Dict[str, Any]]]:
        """
        è¿è¡ŒASRè¯†åˆ«
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            tuple: (input_text, output_text, word_times)
        """
        print(f"ğŸ¤ è¿è¡ŒASRè¯†åˆ«: {audio_path}")
        output = self.asr_model.transcribe([audio_path], timestamps=True)
        
        word_times = [{
            "word": word['word'].strip(",.?!"),
            "start": word['start'],
            "end": word['end']
        } for word in output[0].timestamp['word']]
        
        text = output[0].text
        print(f"ğŸ“ è¯†åˆ«ç»“æœ: {text}")
        return text, text, word_times

    def inpaint_audio(
        self,
        audio_path: str,
        output_text: str,
        num_steps: int = 30,
        init_temp: float = 1.0,
        init_diversity: float = 1.0,
        guidance: float = 0.5,
        rescale: float = 0.7,
        topk: int = 25,
        output_path: Optional[str] = None
    ) -> str:
        """
        éŸ³é¢‘ä¿®å¤/ç¼–è¾‘
        
        Args:
            audio_path: è¾“å…¥éŸ³é¢‘è·¯å¾„
            output_text: ç›®æ ‡æ–‡æœ¬
            num_steps: é‡‡æ ·æ­¥æ•°
            init_temp: åˆå§‹æ¸©åº¦
            init_diversity: åˆå§‹å¤šæ ·æ€§
            guidance: å¼•å¯¼å¼ºåº¦
            rescale: é‡ç¼©æ”¾å› å­
            topk: top-ké‡‡æ ·
            output_path: è¾“å‡ºè·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
            
        Returns:
            str: è¾“å‡ºéŸ³é¢‘è·¯å¾„
        """
        print(f"ğŸ”§ å¼€å§‹éŸ³é¢‘ä¿®å¤...")
        print(f"ğŸ“ è¾“å…¥éŸ³é¢‘: {audio_path}")
        print(f"ğŸ“ ç›®æ ‡æ–‡æœ¬: {output_text}")
        
        # å…ˆè¿è¡ŒASRè·å–åŸå§‹æ–‡æœ¬å’Œæ—¶é—´æˆ³
        input_text, _, word_times = self.run_asr(audio_path)
        
        # è¿è¡Œinpainter
        output_audio = self.inpainter.inpaint(InpaintInput(
            input_text=input_text,
            output_text=output_text,
            input_word_times=word_times,
            audio=audio_path,
            num_steps=num_steps,
            init_temp=init_temp,
            init_diversity=init_diversity,
            guidance=guidance,
            rescale=rescale,
            topk=topk
        ))
        print("æ‰§è¡ŒæˆåŠŸ")
        
        # ä¿å­˜è¾“å‡º
        if output_path is None:
            output_path = f"output_inpaint_{os.path.basename(audio_path)}"
        
        # è¿™é‡Œéœ€è¦æ ¹æ® output_audio çš„ç±»å‹æ¥ä¿å­˜
        # å‡è®¾ output_audio æ˜¯éŸ³é¢‘æ•°æ®ï¼Œéœ€è¦ä¿å­˜ä¸ºæ–‡ä»¶
        print(f"ğŸ’¾ ä¿å­˜è¾“å‡ºéŸ³é¢‘: {output_path}")
        sf.write(output_path, output_audio[1], output_audio[0], subtype='PCM_16')
        
        return output_path

    def text_to_speech(
        self,
        text: str,
        voice_audio_path: str,
        output_path: Optional[str] = None
    ) -> str:
        """
        æ–‡æœ¬è½¬è¯­éŸ³
        
        Args:
            text: è¦è½¬æ¢çš„æ–‡æœ¬
            voice_audio_path: å‚è€ƒè¯­éŸ³è·¯å¾„
            output_path: è¾“å‡ºè·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
            
        Returns:
            str: è¾“å‡ºéŸ³é¢‘è·¯å¾„
        """
        print(f"ğŸ—£ï¸ å¼€å§‹æ–‡æœ¬è½¬è¯­éŸ³...")
        print(f"ğŸ“ è¾“å…¥æ–‡æœ¬: {text}")
        print(f"ğŸµ å‚è€ƒè¯­éŸ³: {voice_audio_path}")
        
        # è¿è¡ŒTTS
        output_audio = self.inpainter.tts(TTSInput(
            output_text=text,
            voice=voice_audio_path
        ))
        
        # ä¿å­˜è¾“å‡º
        if output_path is None:
            output_path = f"output_tts_{os.path.basename(voice_audio_path)}"
        
        print(f"ğŸ’¾ ä¿å­˜è¾“å‡ºéŸ³é¢‘: {output_path}")
        sf.write(output_path, output_audio[1], output_audio[0], subtype='PCM_16')

        return output_path

    def run_complete_pipeline(
        self,
        audio_path: str,
        target_text: str,
        output_path: Optional[str] = None,
        **kwargs
    ) -> str:
        """
        è¿è¡Œå®Œæ•´çš„éŸ³é¢‘ç¼–è¾‘æµç¨‹
        
        Args:
            audio_path: è¾“å…¥éŸ³é¢‘è·¯å¾„
            target_text: ç›®æ ‡æ–‡æœ¬
            output_path: è¾“å‡ºè·¯å¾„
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            str: è¾“å‡ºéŸ³é¢‘è·¯å¾„
        """
        print("ğŸš€ å¼€å§‹å®Œæ•´éŸ³é¢‘ç¼–è¾‘æµç¨‹")
        print("="*50)
        
        result = self.inpaint_audio(
            audio_path=audio_path,
            output_text=target_text,
            output_path=output_path,
            **kwargs
        )
        
        print("="*50)
        print("âœ… éŸ³é¢‘ç¼–è¾‘å®Œæˆ!")
        return result


def main():
    parser = argparse.ArgumentParser(description="PlayDiffusion æ¨ç†è„šæœ¬")
    parser.add_argument("--mode", choices=["inpaint", "tts", "asr"], default="inpaint", 
                       help="è¿è¡Œæ¨¡å¼")
    parser.add_argument("--audio", required=True, help="è¾“å…¥éŸ³é¢‘è·¯å¾„")
    parser.add_argument("--text", help="ç›®æ ‡æ–‡æœ¬ (inpaint/ttsæ¨¡å¼éœ€è¦)")
    parser.add_argument("--voice", help="å‚è€ƒè¯­éŸ³è·¯å¾„ (ttsæ¨¡å¼éœ€è¦)")
    parser.add_argument("--output", help="è¾“å‡ºè·¯å¾„")
    parser.add_argument("--asr_model", default="modeldata/audio/asr/parakeet/parakeet-tdt-0.6b-v2.nemo",
                       help="ASRæ¨¡å‹è·¯å¾„")
    
    # é«˜çº§å‚æ•°
    parser.add_argument("--num_steps", type=int, default=30, help="é‡‡æ ·æ­¥æ•°")
    parser.add_argument("--init_temp", type=float, default=1.0, help="åˆå§‹æ¸©åº¦")
    parser.add_argument("--init_diversity", type=float, default=1.0, help="åˆå§‹å¤šæ ·æ€§")
    parser.add_argument("--guidance", type=float, default=0.5, help="å¼•å¯¼å¼ºåº¦")
    parser.add_argument("--rescale", type=float, default=0.7, help="é‡ç¼©æ”¾å› å­")
    parser.add_argument("--topk", type=int, default=25, help="top-ké‡‡æ ·")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–æ¨ç†å™¨
    inference = PlayDiffusionInference(args.asr_model)
    
    if args.mode == "asr":
        # åªè¿è¡ŒASR
        text, _, word_times = inference.run_asr(args.audio)
        print(f"è¯†åˆ«æ–‡æœ¬: {text}")
        print(f"è¯æ—¶é—´æˆ³: {word_times}")
        
    elif args.mode == "inpaint":
        # éŸ³é¢‘ä¿®å¤æ¨¡å¼
        if not args.text:
            raise ValueError("inpaintæ¨¡å¼éœ€è¦æŒ‡å®š --text å‚æ•°")
        
        result = inference.run_complete_pipeline(
            audio_path=args.audio,
            target_text=args.text,
            output_path=args.output,
            num_steps=args.num_steps,
            init_temp=args.init_temp,
            init_diversity=args.init_diversity,
            guidance=args.guidance,
            rescale=args.rescale,
            topk=args.topk
        )
        print(f"è¾“å‡ºæ–‡ä»¶: {result}")
        
    elif args.mode == "tts":
        # TTSæ¨¡å¼
        if not args.text:
            raise ValueError("ttsæ¨¡å¼éœ€è¦æŒ‡å®š --text å‚æ•°")
        if not args.voice:
            raise ValueError("ttsæ¨¡å¼éœ€è¦æŒ‡å®š --voice å‚æ•°")
        
        result = inference.text_to_speech(
            text=args.text,
            voice_audio_path=args.voice,
            output_path=args.output
        )
        print(f"è¾“å‡ºæ–‡ä»¶: {result}")


if __name__ == "__main__":
    # å¦‚æœç›´æ¥è¿è¡Œï¼Œä½¿ç”¨ç®€å•çš„æµ‹è¯•
    if len(sys.argv) == 1:
        # æµ‹è¯•æ¨¡å¼
        inference = PlayDiffusionInference()
        target_text = "Hey, Erika0920, your instant avatar is ready. Fucking you body. Also, click the feedback button to share what you think. Hope you enjoy. Hey, Erika0920"
        # inference.run_complete_pipeline(
        #     audio_path="notebook/data_source/audio/doc_audio/å¹²å‡€_en.mp3",
        #     target_text=target_text,
        #     output_path="output_inpaint_test.wav",
        # )
        inference.text_to_speech(
            text=target_text,
            voice_audio_path="notebook/data_source/audio/doc_audio/å¹²å‡€_en.mp3",
            output_path="output_tts_test.wav",
        )
    else:
        # å‘½ä»¤è¡Œæ¨¡å¼
        main()